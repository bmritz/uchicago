Week 6 Homework 1 - Logistic Regression
========================================================
Brian Ritz
--------------------------------------------------------
MSCA 31007 Autumn 2014
--------------------------------------------------------
Case 1
-----------------

We will try to unscrample the model generating functions again from Week 4, this time using logistic regression.

Read in the data

```{r}
# import the data
LinearModel.Training <- read.csv("C:/Users/Brian_Ritz/uchicago/statistical_analysis/hw/LinearModelCase1Training.csv")

nSample.Training<-length(LinearModel.Training[,1])
head(LinearModel.Training)

```

Now we also plot the dataset:

```{r fig.width=7, fig.height=6}
plot(LinearModel.Training[,1],LinearModel.Training[,2], type="p",pch=19)
```

Now we separate the dataset based on the values in the thrid column.

```{r}
# Define training samples generated by model 1 and model 2
LinearModel.Training.1<-cbind(LinearModel.Training[,1],rep(NA,nSample.Training))
LinearModel.Training.2<-cbind(LinearModel.Training[,1],rep(NA,nSample.Training))

# this sets the output values for all values where the third column is 1
LinearModel.Training.1[LinearModel.Training[,3]*(1:nSample.Training),2]<-LinearModel.Training[LinearModel.Training[,3]*(1:nSample.Training),2]

# this sets the values for where the third column is 0
LinearModel.Training.2[(1-LinearModel.Training[,3])*(1:nSample.Training),2]<-LinearModel.Training[(1-LinearModel.Training[,3])*(1:nSample.Training),2]

# Print the first 10 rows to check the separation
cbind(LinearModel.Training,LinearModel.Training.1,LinearModel.Training.2)[1:10,]

```

So now we have two datasets that represent the observations from the two model switch variables.

Plot the subsamples:

```{r}
# Plot the subsamples
matplot(LinearModel.Training[,1],cbind(LinearModel.Training.1[,2],LinearModel.Training.2[,2]),pch=16,col=c("green","blue"),ylab="Subsamples of the training sample")
```

Lets estimate a linear regression line for all of the data we just plotted.

```{r}
EstimatedLinearModel.Training<-lm(LinearModel.Training[,2]~LinearModel.Training[,1])
EstimatedLinearModel.Training$coefficients

```

Let's see the summary.

```{r}
summary(EstimatedLinearModel.Training)
```

**INTERPRETATION**
The p-value of the intercept is .317, indicating that we do not have a lot of confidence that the true intercept of the model is different from 0. The parameter estimate of the X variable is .79332, indicating that for every increase in the first variable of magnitude 1, the 2nd variable increases by ~.79. The R squared for the model is .84, indicating that 84% of model total variance can be explained by differences in the model inputs.


Let's check out the residuals.
```{r}
EstimatedResiduals.Training<-EstimatedLinearModel.Training$residuals
plot(LinearModel.Training[,1],EstimatedResiduals.Training)
```

Plot residuals for observations from the two different models (as indicated by that third variable):

```{r}
# Define residuals corresponding to different models 
EstimatedResiduals.Training.1<-EstimatedResiduals.Training
EstimatedResiduals.Training.2<-EstimatedResiduals.Training
EstimatedResiduals.Training.1[(LinearModel.Training[,3]==0)*(1:nSample.Training)]<-NA
EstimatedResiduals.Training.2[(LinearModel.Training[,3]==1)*(1:nSample.Training)]<-NA
# Print the first ten columns to check the separation 
cbind(EstimatedResiduals.Training,EstimatedResiduals.Training.1,EstimatedResiduals.Training.2,LinearModel.Training[,3])[1:10,]
```


Plot the residuals again, and coloring them by model. 
```{r}
# Plot the residuals corresponding to different models
matplot(LinearModel.Training[,1],cbind(EstimatedResiduals.Training.1,EstimatedResiduals.Training.2),pch=16,col=c("green","blue"),ylab="Separated parts of the training sample")
```

The residuals for the first model (the model with 1's in the third variable) are mostly greater than 0, while the residuals for the second model are mostly negative. This means that the first group of observations mostly lie above the regression line, while the second group of observations lie below the regression line. This means that the two groups of observations can most likely should be estimated by two different lines.


**Now we try to separate the clusters of residuals using Logistic Regression**

```{r}
# Create the data frame for logistic regression
Logistic.Model.Data<-data.frame(Logistic.Output=LinearModel.Training[,3],Logistic.Input=EstimatedResiduals.Training)
LinearModel.Training.Logistic<-glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data,family=binomial(link=logit))
summary(LinearModel.Training.Logistic)
```

I use the predict function to find the probability of the first model.

```{r}
Predicted.Probabilities.Training<-predict(LinearModel.Training.Logistic,type="response")
plot(LinearModel.Training[,1],Predicted.Probabilities.Training)

```

This graph shows that most observations are fairly well predicted. They either have a very high probability or very low probability of belonging to the first model. This means that the model is not "unsure" about the majority of the observations. 


Now lets use that predicted values from the logistic regression to determine which point belongs to which point. We partition out the residuals from the original model according to which group we estimate the model belonging to. Then we can use those values further in the analysis to determine two models for the two different groups of observations.

```{r}
# Create the unscrambling sequence for the training sample
Unscrambling.Sequence.Training.Logistic<-(predict(LinearModel.Training.Logistic,type="response")>.5)*1
# Create classified residuals
ClassifiedResiduals.Training.1<-EstimatedResiduals.Training
ClassifiedResiduals.Training.2<-EstimatedResiduals.Training
ClassifiedResiduals.Training.1[(Unscrambling.Sequence.Training.Logistic==0)*(1:nSample.Training)]<-NA
ClassifiedResiduals.Training.2[(Unscrambling.Sequence.Training.Logistic==1)*(1:nSample.Training)]<-NA

# we now have the residuals by group as estimated by our logistic model
# Print first 10 rows to check
cbind(EstimatedResiduals.Training,ClassifiedResiduals.Training.1,ClassifiedResiduals.Training.2)[1:10,]
```

```{r}
# Plot both classes of the residuals
matplot(LinearModel.Training[,1],cbind(ClassifiedResiduals.Training.1,ClassifiedResiduals.Training.2),pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at 0")
axis(1,pos=0)
```


```{r}
Classification.Rule.Logistic<--LinearModel.Training.Logistic$coefficients[1]/LinearModel.Training.Logistic$coefficients[2]
Classification.Rule.Logistic
```

The classification rule we used previously in assignment 4 is whether or not the residual from the original model was greater than or equal to zero.

The classification rule estimated by logistic regression is whether or not the residual is above or below -0.0738.

```{r}
matplot(LinearModel.Training[,1],cbind(ClassifiedResiduals.Training.1,ClassifiedResiduals.Training.2),pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)
```


**NOW LETS APPLY THAT TO THE TRAINING DATASET**
Read in the sample.

```{r}
LinearModel<-read.csv(file="C:/Users/Brian_Ritz/uchicago/statistical_analysis/hw/LinearModelCase1.csv",header=TRUE,sep=",")

nSample<-length(LinearModel[,1])
LinearModel[1:10,]
```


Estimated linear model from the main sample
```{r}
EstimatedLinearModel<-lm(LinearModel[,2]~LinearModel[,1])
EstimatedLinearModel$coefficients

```

Get the residuals.

```{r}
EstimatedResiduals<-EstimatedLinearModel$residuals
plot(LinearModel[,1],EstimatedResiduals)
```


Define the predicted probabilities and unscrambling sequence for the main sample.
Note: the function predict uses the argument newdata. This argument should be a data frame with the same names of variables as the data frame used to estimate logistic model. But the residuals in this data frame are from the main sample.

```{r}
Unscrambling.Sequence.Logistic<-(predict(LinearModel.Training.Logistic,newdata=data.frame(Logistic.Output=EstimatedResiduals,Logistic.Input=EstimatedResiduals),type="response")>.5)*1
```

Estimate probability of the model classified as first. Check the hypothesis p=0.5 against two-sided alternative.

```{r}
Probability<-sum(Unscrambling.Sequence.Logistic)/length(Unscrambling.Sequence.Logistic)
Probability

```

```{r}
binom.test(sum(Unscrambling.Sequence.Logistic),nSample,p=.5,alternative="t")
```

We see from the binomial test that we cannot rule out that the sequence is the result of a simple bernoulli experiment with p=.5.

Classify the residuals of the main sample into 2 groups using the logistic model classifier.

```{r}
# Create classified residuals
ClassifiedResiduals.1<-EstimatedResiduals
ClassifiedResiduals.2<-EstimatedResiduals
ClassifiedResiduals.1[(Unscrambling.Sequence.Logistic==0)*(1:nSample)]<-NA
ClassifiedResiduals.2[(Unscrambling.Sequence.Logistic==1)*(1:nSample)]<-NA
# Print first 10 rows to check
cbind(EstimatedResiduals,ClassifiedResiduals.1,ClassifiedResiduals.2)[1:10,]
```

```{r}
# Plot both classes of the residuals
matplot(LinearModel[,1],cbind(ClassifiedResiduals.1,ClassifiedResiduals.2),pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at 0")
axis(1,pos=0)
```

```{r}
matplot(LinearModel[,1],cbind(ClassifiedResiduals.1,ClassifiedResiduals.2),pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)
```

Both residuals look good -- no patterns.

Separate the given sample into 2 subsamples using the trained logistic model.

```{r}
# Create recovered models
LinearModel1.Recovered<-LinearModel
LinearModel2.Recovered<-LinearModel
LinearModel1.Recovered[(1-Unscrambling.Sequence.Logistic)*(1:nSample),2]<-NA
LinearModel2.Recovered[Unscrambling.Sequence.Logistic*(1:nSample),2]<-NA
# Print the first 1 rows of scrambled and unscrambled samples
cbind(LinearModel,LinearModel1.Recovered,LinearModel2.Recovered)[1:10,]
```

```{r}
# Plot the unscrambled subsamples
matplot(LinearModel[,1],cbind(LinearModel1.Recovered[,2],LinearModel2.Recovered[,2]), type="p",col=c("green","blue"),pch=19,ylab="Separated Subsamples")
```

Now estimate the linear models from the subsamples.
```{r}
LinearModel1.Recovered.lm<-lm(LinearModel1.Recovered[,2]~LinearModel1.Recovered[,1])
LinearModel2.Recovered.lm<-lm(LinearModel2.Recovered[,2]~LinearModel2.Recovered[,1])
```

Compare the results of fitting of the first recovered linear model:
```{r}
summary(LinearModel1.Recovered.lm)
```

and the second recovered linear model:
```{r}
summary(LinearModel2.Recovered.lm)
```

The results of the first and second linear model differ differ in their estimate of the intercept. Both of the models' estimates for the slope parameters is not significantly different from zero, so we cannot rule out that their slopes are the same. However, model1's estimate of the intercept is 4.88, while model2's estimate of the intercept is materially different, coming in at 1.0717.



Compare the summaries of the mix with the summary of the single linear model fit.
```{r}
# summary of single linear model fit
summary(EstimatedLinearModel)
```
The single model linear fit had a near 0 parameter for the effect of the input variable, with a non-zero parameter for the intercept. This differs from the two estimates of two models unscrambled according to logistic regression. In the two models method, we found the intercepts to both be non-distinguishable from 0, and the coefficients on the input variable to be greater than zero in both cases.


Plot the residuals estimated by a single linear model and the residuals of the unscrambled mix. Estimate standard deviations of the two samples of residuals:

```{r}
# Plot residuals
Residuals.Comparison<-cbind(Unscrambled.residuals=c(summary(LinearModel1.Recovered.lm)$residuals,summary(LinearModel2.Recovered.lm)$residuals),Single.Model.residuals=EstimatedResiduals)
matplot(Residuals.Comparison,type="p",pch=16,ylab="Residuals before and after unscrabling")

```

```{r}
# Estimate standard deviations
apply(Residuals.Comparison,2,sd)
```

The sample was mixed using estimated binomial probability of `probability`

Binomial test does not reject the hypothesis p=0.5 against the two-sided alternative.

The mixed models have the following estimated parameters:
```{r}
rbind(LinearModel1.Recovered.lm$coefficients,LinearModel2.Recovered.lm$coefficients)
```

  