len(j)
length(j)
j
?table
t <- read.csv('C:/Users/Brian_Ritz/stubhub/output/master_tickets.csv', header=T)
summary(t)
table(t$run_id)
nobs(t)
?rows
?nrows
?nrow
nrow(t)
nrow(t$run_id)
length(t$run_id)
table(t$run_id)
length(t[t$run_id == NA])
length(t[t$run_id == NA,1])
length(t[t$run_id == NA,])
t[t$run_id == NA]
t[t$run_id == NA,]
train<- read.table(“C:/kaggle/valued_shoppers/output/training_dset.csv”,
header = TRUE, sep = “,”)
train<- read.table(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, sep = “,”)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, sep = “,”)
?read.csv
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, sep = “,”)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, sep = “,”)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, sep = “,”)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE)
?read.csv
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE)
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, nrows=4)
a = 'this is a string'
train <- read.csv(“C:/kaggle/valued_shoppers/output/training_dset.csv”, header = TRUE, nrows=4, encoding="UTF-8")
train <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header=TRUE)
train <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
?runif
runif(2,0,1)
len(train)
length(train)
train
nrows(train)
nrow(train)
train$randvar <- runif(nrow(train), 0, 1)
apply(random.vector, 1, function (x) if x > .8 return 1 else return 0)
makeCV <- function(x){
if (x > .8){
return 1
}else{
return 0
}
}
makeCV <- function(x){
if (x > .8){
return(1)
}else{
return(0)
}
}
apply(random.vector, 1, makeCV)
random.vector <- runif(nrow(train), 0, 1)
apply(random.vector, 1, makeCV)
?apply
sapply(random.vector, makeCV)
cv_flag <- sapply(random.vector, makeCV)
cbind(random.vector, cv_flag)
train <- cbind(train, cv_flag)
cols(train)
columns(train)
train
View(train)
mean(train$cv_flag)
#read the training data into a dataframe called train
train <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
# create training or cv dataset variables
random.vector <- runif(nrow(train), 0, 1)
makeCV <- function(x){
if (x > .8){
return(1)
}else{
return(0)
}
}
cv_flag <- sapply(random.vector, makeCV)
train <- cbind(train, cv_flag)
?set.seed
#read the training data into a dataframe called train
train <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
set.seed(4)
# create training or cv dataset variables
random.vector <- runif(nrow(train), 0, 1)
makeCV <- function(x){
if (x > .8){
return(1)
}else{
return(0)
}
}
cv_flag <- sapply(random.vector, makeCV)
train <- cbind(train, cv_flag)
colnames(train)
logit.model <- glm(dependent_var~less_than_30+
id_company_category_brand_purch_flag+
time_since_purch+
id_category_price_index,
data=train[train$cv_flag=0,],
family=binomial)
summary(logit.model)
logit.model <- glm(dependent_var~less_than_30+
id_company_category_brand_purch_flag+
time_since_purch+
id_category_price_index,
data=train[train$cv_flag=0,],
family='binomial')
summary(logit.model)
logit.model <- glm(dependent_var~less_than_30+id_company_category_brand_purch_flag+time_since_purch+id_category_price_index,
data=train[train$cv_flag=0,],
family='binomial')
logit.model <- glm(dependent_var~less_than_30+id_company_category_brand_purch_flag+time_since_purch+id_category_price_index,
data=train[train$cv_flag==0,],
family='binomial')
summary(logit.model)
train$less_than_30
train$id_company_category_brand_purch_flag
sum(train$time_since_purch==NA)
sum(train$time_since_purch==1)
sum(train$time_since_purch==2)
train$id_category_price_index
logit.model <- glm(dependent_var~less_than_30+id_company_category_brand_purch_flag+time_since_purch+id_category_price_index,
data=train[train$cv_flag==0,],
family='binomial')
logit.model <- glm(dependent_var~less_than_30+id_company_category_brand_purch_flag+time_since_purch,
data=train[train$cv_flag==0,],
family='binomial')
summary(logit.model)
?glm
logit.model <- glm(dependent_var~less_than_30+id_company_category_brand_purch_flag+time_since_purch+id_category_price_index,
data=train[train$cv_flag==0,],
na.action="na.pass"
family='binomial')
summary(logit.model)
colnames(train)
a <-  c(4,4,4,NA, 4,4,NA,4,NA,NA)
a[a==NA]
a==NA
is.na(a)
a[is.na(a)] <-'this'
a
price.index <- train$id_company_category_brand_price_index
price.index[is.na(price.index)] <- 1
price_index
price.index
train$id_company_category_brand_price_index <- price.index
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch,id_company_category_brand_price_index
data=train[train$cv_flag==0,],
family='binomial')
summary(logit.model)
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
family='binomial')
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = "na.pass"
family='binomial')
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = na.pass
family='binomial')
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = 'na.pass',
family='binomial')
sum(is.na(train$id_company_category_price_index))
price.index <- train$id_company_category_brand_price_index
price.index[is.na(price.index)] <- 1
train$id_company_category_brand_price_index <- price.index
sum(is.na(train$id_company_category_price_index))
train$id_company_category_brand_price_index
sum(is.na(train$id_company_category_brand_price_index))
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = 'na.pass',
family='binomial')
train$id_company_category_brand_price_index
sum(is.na(train$id_company_category_brand_price_index))
is.nan
sum(is.nan(train$id_company_category_brand_price_index))
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = 'na.pass',
family='binomial')
is.inf
sum(is.finite(train$id_company_category_brand_price_index))
length(train$id_company_category_brand_price_index)
train <- train[is.finite(train$id_company_category_brand_price_index)]
train <- train[is.finite(train$id_company_category_brand_price_index),]
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = 'na.pass',
family='binomial')
summary(logit.model)
cvdat <- train[train$cv_flag==1,]
?predict
redeem_prob <- predict(logit.model, newdata=cvdat, type='response')
pred_label <- ifelse(redeem_prob>.5, 1, 0)
confuse_mat <- table(actual<-cvdat$dependent_var, predicted=pred_label)
fix(confuse_mat)
confuse_mat
confuse_mat <- table(actual<-cvdat$dependent_var, predicted=pred_label)
accuracy = sum(confuse_mat[1,1],confuse_mat[2,2])/sum(confuse_mat)
recall = confuse_mat[2,2] / sum(confuse_mat[2,])
precision = confuse_mat[2,2] / sum(confuse_mat[,2])
f1_score = 2*(recall*precision/(recall+precision))
accuracy
mean(train$dependent_var)
auc = mean(sample(redeem_prob[cvdat$dependent_var==1],100000,replace=T) >   sample(redeem_prob[cvdat$dependent_var==0],100000,replace=T))
auc
#read the training data into a dataframe called train
train <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
set.seed(4)
# create training or cv dataset variables
random.vector <- runif(nrow(train), 0, 1)
cv_flag <- sapply(random.vector, makeCV)
train <- cbind(train, cv_flag)
# change NAs to 1s in the index
price.index <- train$id_company_category_brand_price_index
price.index[is.na(price.index)] <- 1
train$id_company_category_brand_price_index <- price.index
# throw out infinite price indicies
train <- train[is.finite(train$id_company_category_brand_price_index),]
# fit the model
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = 'na.pass',
family='binomial')
summary(logit.model)
# cross validate
cvdat <- train[train$cv_flag==1,]
redeem_prob <- predict(logit.model, newdata=cvdat, type='response')
pred_label <- ifelse(redeem_prob>.5, 1, 0)
# see results
confuse_mat <- table(actual<-cvdat$dependent_var, predicted=pred_label)
accuracy = sum(confuse_mat[1,1],confuse_mat[2,2])/sum(confuse_mat)
recall = confuse_mat[2,2] / sum(confuse_mat[2,])
precision = confuse_mat[2,2] / sum(confuse_mat[,2])
f1_score = 2*(recall*precision/(recall+precision))
auc = mean(sample(redeem_prob[cvdat$dependent_var==1],100000,replace=T) >   sample(redeem_prob[cvdat$dependent_var==0],100000,replace=T))
#read the training data into a dataframe called train
train <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
set.seed(4)
# create training or cv dataset variables
random.vector <- runif(nrow(train), 0, 1)
makeCV <- function(x){
if (x > .8){
return(1)
}else{
return(0)
}
}
cv_flag <- sapply(random.vector, makeCV)
train <- cbind(train, cv_flag)
# change NAs to 1s in the index
price.index <- train$id_company_category_brand_price_index
price.index[is.na(price.index)] <- 1
train$id_company_category_brand_price_index <- price.index
# throw out infinite price indicies
train <- train[is.finite(train$id_company_category_brand_price_index),]
# fit the model
logit.model <- glm(dependent_var~id_company_category_brand_purch_flag+time_since_purch+id_company_category_brand_price_index,
data=train[train$cv_flag==0,],
na.action = 'na.pass',
family='binomial')
# cross validate
cvdat <- train[train$cv_flag==1,]
redeem_prob <- predict(logit.model, newdata=cvdat, type='response')
pred_label <- ifelse(redeem_prob>.5, 1, 0)
confuse_mat <- table(actual<-cvdat$dependent_var, predicted=pred_label)
accuracy = sum(confuse_mat[1,1],confuse_mat[2,2])/sum(confuse_mat)
recall = confuse_mat[2,2] / sum(confuse_mat[2,])
precision = confuse_mat[2,2] / sum(confuse_mat[,2])
f1_score = 2*(recall*precision/(recall+precision))
auc = mean(sample(redeem_prob[cvdat$dependent_var==1],100000,replace=T) >   sample(redeem_prob[cvdat$dependent_var==0],100000,replace=T))
test.data <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE))
test.data <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
?predict
test_predictions <- predict(logit.model, newdata=test.data, type='response')
submission <- test.data['id',]
View(submission)
submission <- test.data[,'id']
fix(submission)
submission <- cbind(id=test.data[,'id'], repeatProbability=test_predictions)
?cbind
test.data <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
test_predictions <- predict(logit.model, newdata=test.data, type='response')
submission <- cbind(test.data[,'id'], test_predictions)
?data.frame
test <-  data.frame(id=test.data[,'id'], repeatProbability=test_predictions)
View(test)
test.data[~is.finite(test.data$id_company_category_brand_price_index),]
a <- test.data[~is.finite(test.data$id_company_category_brand_price_index),]
a <- test.data[is.finite(test.data$id_company_category_brand_price_index),]
a
a <- test.data[is.finite(test.data$id_company_category_brand_price_index)==0,]
test.data[,'id_company_category_brand_price_index']
test.data[is.finite(test.data$id_company_category_brand_price_index)==0,'id_company_category_brand_price_index'] <- 1
test.data[,'id_company_category_brand_price_index']
test.data <- read.csv("C:/kaggle/valued_shoppers/output/training_dset.csv", header= TRUE)
# fix some of the test data
price.index <- test.data$id_company_category_brand_price_index
price.index[is.na(price.index)] <- 1
test.data$id_company_category_brand_price_index <- price.index
test.data[is.finite(test.data$id_company_category_brand_price_index)==0,'id_company_category_brand_price_index'] <- 1
test_predictions <- predict(logit.model, newdata=test.data, type='response')
submission <- data.frame(id=test.dat[,'id'], repeatProbability=test_predictions)
submission <- data.frame(id=test.data[,'id'], repeatProbability=test_predictions)
?write.csv
write.csv(submission, "C:/kaggle/valued_shoppers/output/submission1.csv")
write.csv(submission, "C:/kaggle/valued_shoppers/output/submission1.csv", row.names=FALSE)
library(TTR)
install.packages('TTR')
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rainseries <- ts(rain,start=c(1813))
plot.ts(rainseries)
rainseriesforecasts <- HoltWinters(rainseries, beta=FALSE, gamma=FALSE)
?HoltWinters
ts
?ts
R.version()
R.Version()
print("hello")
1 + 0.2 * (3e2 - 4) / 5e-1 # Integer, decimal, and exponential number.
sqrt(3 ** 2 + 4 ^ 2) # Both ** and ^ are exponent.
12e4
cos(5)
cos(.5)
!is.null(NULL)
is.null(NULL)
answer <- total <= 10
answer <- total <= 10
answer
?save.image
install.packages('knitr')
numbers <- c(1, -2, 3) # Here "c" is a built-in function to combine items.
phones <- c("Apple", "Samsung", "Nokia", "HTC")
-c(4,5)
x <- c(1,2,3,4)
y <- c(5,10)
x / y
blood.type <- c("O", "A", "B", "A", "O", "AB", "A", "B")
f <- factor(blood.type) # Convert a vector to factor with levels.
levels(f)
str(blood.type)
str(f)
names(f)
as.character(f)
f
getwd()
?setwd
setwd("C:\Users\Brian_Ritz\uchicago\r_workshop\input_files")
setwd("C:/Users/Brian_Ritz/uchicago/r_workshop/input_files")
read.csv('hflights.csv')
hflights <- read.csv('hflights.csv')
names(hflights)
hflights
head(hflights)
colnames(hflights)
# (Finally, we have learned enough of the boring technical stuff to perform
# some useful statistical analyses.)
hflights <- read.csv("hflights.csv")
arrival.delay <- hflights$ArrDelay
arrival.delay[1:5]
summary(arri)
summary(arrival.delay)
is.null(NA)
is.na(NA)
sd(arrival.delay)
sd(arrival.delay[arrival.delay != NA])
arrival.delay != NA
sd(arrival.delay[!is.na(arrival.delay))
sd(arrival.delay[!is.na(arrival.delay)])
?which
idx <- which.max(arrival.delay)
ids
idx
arrival.delay[idx]
hflights[idx,]
is.integer(NA)
is.integer(3)
?is.integer
is.float(3)
class(#)
jlkasj
class(3)
is.numeric(NA)
is.numeric(4)
is.numeric(NULL)
sum(is.na(arrival.delay))
?any
any(is.na(arrival.delay))
any(c(FALSE, FALSE, FALSE))
any(c(FALSE, FALSE, TRUE))
?complete.cases
complete.cases(hflights)
hflights.clean <- hflights[complete.cases(hflights),]
c(nrow(hflights), nrow(hflights.clean))
nrow(hflights) - nrow(hflights.clean)
sum(is.na(arrival.delay))
idx <- order(hflights.clean$Distance, hflights.clean$AirTime)
summary(arrival.delay) # See also: min, max, mean, median, range, quantile
sd(arrival.delay) # Fail because of missing values i.e. NA.
hflights.clean <- hflights.clean[idx,]
hflights <- read.csv("hflights.csv")
# Task 1: Arrival delay.
arrival.delay <- hflights$ArrDelay
summary(arrival.delay) # See also: min, max, mean, median, range, quantile
sd(arrival.delay) # Fail because of missing values i.e. NA.
# See also: var
sd(arrival.delay, na.rm=TRUE) # Ignore missing values.
idx <- which.max(arrival.delay) # See also: which.min
hflights[idx,]
# Task 2: Cleaning missing data.
x <- c(1, 2, NA, 4)
is.na(x)
any(is.na(arrival.delay)) # See also: all
sum(is.na(arrival.delay)) # How many NA's are there?  TRUE is 1 and FALSE is 0.
mean(is.na(arrival.delay)) # Fraction of NA's.
is.complete <- complete.cases(hflights) # Vector of TRUE or FALSE.
# See also: na.omit
hflights.clean <- hflights[is.complete,] # Indexing by boolean values.
c(nrow(hflights), nrow(hflights.clean))
str(hflights)
num.flight <- table(hflights$UniqueCarrier)
class(numflight)
class(num.flight
)
num.flight$AA
num.flight[1]
rank(-num.flight, ties.method="first")
order(rank(-num.flight, ties.method="first")
)
sort(rank(-num.flight, ties.method="first"))
table(hflights.clean$UniqueCarrier, hflights.clean$Origin)
delay <- hflights.clean$DepDelay
airport <- hflights.clean$Origin
length(delay)
head(ai)
airport
hs.delay <- factor(delay > 0)
has.delay <- factor(delay > 0)
str(has.delay)
levels(has.delay) <- c("on_time", "delay")
str(has.delay)
result <- table(airport, has.delay)
?table
result
prop.table(result, margin=1)
?prop.table
prop.table(result, margin=2)
prop.table(result, margin=3)
prop.table(result, margin=0)
prop.table(result)
pos.delay.per.airport <- split(delay[delay>0], airport[delay>0])
str(pos.delay.per.airport)
class(pos.delay.per.airport)
lapply(pos.delay.per.airport, summary)
hou.pos.delay <- pos.delay.per.airport$HOU
iah.pos.delay <- pos.delay.per.airport$IAH
t.test(hou.pos.delay, iah.pos.delay) # Two-sample t-test.
www.statmethods.net/advgraphs # Fine-tuning and other tools.
"
# Useful websites.
www.statmethods.net/graphs # Various kinds of graphs.
www.statmethods.net/advgraphs # Fine-tuning and other tools.
"
hist(rnorm(1e5))
barplot(num.flights,)
barplot(num.flights,)
barplot(num.flights,main="MAIN TITLE")
barplot(num.flight, main="MAIN TITLE")
?barpot
?barplot
?density
install.packages('dplyr')
# Task 1: Use "dplyr" package to do split-apply-combine.
library(dplyr) # Load a package that has already been installed.
hflights.clean <- na.omit(read.csv("hflights.csv"))
?tbl_df
library(ggplot2)
ggplot(planes, aes(dist, delay)) +
geom_point(aes(size = count), alpha = 1/2) +
geom_smooth() +
scale_size_area()
hflights.clean <- na.omit(read.csv("hflights.csv"))
planes <- tbl_df(hflights.clean) %.% # Special syntax for "dplyr".
group_by(TailNum) %.% # Group data by airplane.  Similar to split().
summarize( # Compute some statistics for each airplane.  Similar to lapply(...).
count = n(),
dist = mean(Distance),
delay = mean(ArrDelay)) %.%
filter(count > 20, dist < 2000) # Select those rows that satisfy these criterions.
planes
# Task 2: Use "ggplot2" package to plot.
library(ggplot2)
ggplot(planes, aes(dist, delay)) +
geom_point(aes(size = count), alpha = 1/2) +
geom_smooth() +
scale_size_area()
?paste0
LETTERS
numbers
