---
title: "Course Project"
author: "Brian Ritz"
date: "Saturday, February 14, 2015"
output: html_document
---

1. Problem Description

The business analytics group of a company is asked to investigate causes of malfunctions in technological process of one of the manufacturing plants that result in significantly increased cost to the end product of the business.
One of suspected reasons for malfunctions is deviation of temperature during the process from optimal levels. The sample in the provided file contains times of malfunctions in seconds since the start of measurement and minute records of temperature.


2. Data -- Read in and prepare the data:
```{r}

Course.Project.Data<-read.csv(file="../input/MScA_LinearNonLinear_MalfunctionData.csv")
Course.Project.Data<-as.data.frame(Course.Project.Data)
Course.Project.Data[1:20,]
dim(Course.Project.Data)
```


3. Create Counting Process, Explore Cumulative Intensity
```{r}
Counting.Process<-as.data.frame(cbind(Time=Course.Project.Data$Time,Count=1:length(Course.Project.Data$Time)))
Counting.Process[1:20,]
```

```{r}
plot(Counting.Process$Time,Counting.Process$Count,type="s")
```


3.1 Explore the Cumulative intensity of the process

Cumulative intensity is calculated as the number of events between time zero and t divided by t. 
```{r}
plot(Counting.Process$Time,Counting.Process$Count/Counting.Process$Time,type="l",ylab="Cumulative Intensity")
abline(h=Counting.Process$Count[length(Counting.Process$Count)]/
         Counting.Process$Time[length(Counting.Process$Time)])
abline(h=mean(Counting.Process$Count/Counting.Process$Time))
```

```{r}
c(Last.Intensity=Counting.Process$Count[length(Counting.Process$Count)]/
         Counting.Process$Time[length(Counting.Process$Time)],
  Mean.Intensity=mean(Counting.Process$Count/Counting.Process$Time))
```

4. Check for overdispersion

Make 60 minute windows  -- how many counts are there in each one minute window...one dot is one observation for a 60 minute period.
```{r}
Event.Counts<-hist(ceiling(Counting.Process$Time/60), breaks=15000/60)$counts

plot(Event.Counts)
```

4.1 Methods for testing overdispersion:

4.1.1 Quick and dirty method.
Look at the output of glm() and compare the residual deviance with the number of degrees of freedom.
If the assumed model is correct deviance is asymptotically distributed as Chi-squared (X2) with degrees of freedom n−k where n is the number of observations and k is the number of parameters.
For Chi-squared distribution the mean is the number of degrees of freedom n−k.
If the residual deviance returned by glm() is greater than n−k then it might be a sign of overdispersion.

if no overdipersion -- then expectation of deviance should be near the residual degress of freedom -- if we divide both and there is no overdispersion, then we should get something near 1.

Test the method on simulated Poisson data.
```{r}

# use distribution that we know is poisson and test function will give us what we think
Test.Deviance.Overdispersion.Poisson<-function(Sample.Size,Parameter.Lambda){
  my.Sample<-rpois(Sample.Size,Parameter.Lambda) # get poisson dist
  Model<-glm(my.Sample~1,family=poisson) # fit poisson wth only intercept
  Dev<-Model$deviance  # get deviance
  Deg.Fred<-Model$df.residual # get deg freedom
  
  # create confidence interval -- and retrun if the deg freedom is within this confidence interval
  (((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96))*1
} 

# return 1 if confidence interval does cover "0" in the interval, 0 if it does not
Test.Deviance.Overdispersion.Poisson(100,1)
```


```{r}
sum(replicate(300,Test.Deviance.Overdispersion.Poisson(100,1)))
```


```{r}
exp(glm(rpois(1000,2)~1,family=poisson)$coeff)
```


Same test on negative binomial data:

```{r}
Test.Deviance.Overdispersion.NBinom<-function(Sample.Size,Parameter.prob){
  my.Sample<-rnbinom(Sample.Size,2,Parameter.prob)
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  (((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96))*1
} 
sum(replicate(300,Test.Deviance.Overdispersion.NBinom(100,.2)))

```

Apply the test to one minute counts
```{r}
GLM.model<-glm(Event.Counts~1,family=poisson)
GLM.model
```
The residual deviance is about 6 times n-k (the degrees of freedom). This is a signal of overdispersion.


4.1.2 Regression test by Cameron-Trivedi

Use the AER package to test the hypothesis that the variance is equal to the mean.
This first tests that dispersiontest gives us the expected results. Then we use the dispersiontest to test GLM.model.
```{r}
library(AER)

Test.Deviance.Overdispersion.Poisson.AER<-function(Sample.Size,Parameter.Lambda){
  my.Sample<-rpois(Sample.Size,Parameter.Lambda) # get poisson dist
  Model<-glm(my.Sample~1,family=poisson) # fit poisson wth only intercept
  Disp.Test <- dispersiontest(Model)
  return(Disp.Test)
} 

Test.Deviance.Overdispersion.Poisson.AER(100, .2)

Disp.Test <- dispersiontest(GLM.model)
Disp.Test

```
Given the p-value of < 2.2e-16, we reject the null hypothesis that the mean is equal to the variace, this is consistent with our earlier findings.


4.1.3 Test against Negative Binomial Distribution
The null hypothesis of this test is that the distribution is Poisson as particular case of Negative binomial against Negative Binomial. 

```{r}
library(MASS)
library(pscl)
```

Test the validitiy of the odtest.


Use odTest to test the glm.nb model created by the mass package 
```{r}
GLM.model.nb<-glm.nb(Event.Counts~1)
GLM.model.nb
odTest(GLM.model.nb)

```
Again, we see that the p-value is very small -- indicating overdispersion.



5. Distribution of the Poisson Intensity
Kolmlgorov-Smirnov test -- tests if samples come from same distribution


```{r}  
library(lattice)
library(latticeExtra)
```

```{r}
sample1=rnorm(100)
sample2=rnorm(100,1,2)
Cum.Distr.Functions <- data.frame(sample1,sample2)
ecdfplot(~ sample1 + sample2, data=Cum.Distr.Functions, auto.key=list(space='right'))
```
```{r}
ks.test(sample1,sample2)
```
P-value is again very small -- this is another indication that we may have overdispersion.


Check eqiovalence of empirical distribution of sample1 and theoretical distribution Norm(0,1).
The null hypothesis is that the distributions are the same.
```{r}
ks.test(sample1,"pnorm",mean=0,sd=1)
```
There is weak evidence that sample 1 is different from a normal distribution with mean 0 and standard deviation 1.

Check equivalence of the empirical distribution of sample2 and theoretical distribution Norm(0,1).
```{r}
ks.test(sample2,"pnorm",mean=0,sd=1)
```
It appears that sample2 is significantly different from the normal distribution.

5.2. Check the distribution for the entire period.
if it is a poisson distribution, then the time intervals between malfunctions should be exponential
```{r}
time.differences <- diff(Counting.Process$Time)
ks.test(time.differences, "pexp", rate=1/mean(time.differences))

# plot a CDF of the time differences
ecdfplot(~ time.differences)
```

5.3. Check distribution of one-minute periods

Use at least 5 different candidates for distribution of Poisson intensity of malfunctions.

Find one-minute intensities.
```{r}

Event.Intensities <-hist(ceiling(Counting.Process$Time/60), breaks=15000/60)$counts
Event.Intensities <-  Event.Intensities/60
```
```{r}
# event intensities are the counts of observations in each 1 minute period
hist(Event.Intensities)
```

1st Distribution:
Normal
```{r}
normal.fitting <- fitdistr(Event.Intensities, "normal")
print(normal.fitting)
```

2nd Distribution 
Exponential
```{r}
exponential.fitting <- fitdistr(Event.Intensities, "exponential")
print(exponential.fitting)
```

3rd Distribution
Geometric
```{r}
geometric.fitting <- fitdistr(Event.Intensities, "geometric")
print(geometric.fitting)
```

4th Distribution
Beta
```{r}
beta.fitting <- fitdistr(Event.Intensities[Event.Intensities>0], "beta", list(shape1=.5, shape2=.5))
print(beta.fitting)
```

5th Distribution
Gamma
```{r}
gamma.fitting <- fitdistr(Event.Intensities[Event.Intensities>0], "gamma",list(shape = 1, rate = 0.1), lower = 0.001, upper=.999)
print(gamma.fitting)
```

Test the fitted distributions with the Kolmogoraov-Smirnov Test
Normal:
```{r}
KS.Normal <- ks.test(Event.Intensities, "pnorm", mean=normal.fitting$estimate["mean"], sd=normal.fitting$estimate["sd"])
c(KS.Normal$statistic, P.Value=KS.Normal$p.value)
```

Exponential:
```{r}
KS.exp <- ks.test(Event.Intensities, "pexp", rate=exponential.fitting$estimate["rate"])
c(KS.exp$statistic, P.Value=KS.exp$p.value)
```

Geometric:
```{r}
KS.geom <- ks.test(Event.Intensities, "pgeom", prob=geometric.fitting$estimate["prob"])
c(KS.geom$statistic, P.Value=KS.geom$p.value)
```

Beta:
```{r}
KS.beta <- ks.test(Event.Intensities, "pbeta", shape1=beta.fitting$estimate["shape1"],  shape2=beta.fitting$estimate["shape2"])
c(KS.beta$statistic, P.Value=KS.beta$p.value)
```

Gamma:
```{r}
KS.gamma <- ks.test(Event.Intensities, "pgamma", shape=gamma.fitting$estimate["shape"], rate=gamma.fitting$estimate["rate"])
c(KS.gamma$statistic, P.Value=KS.gamma$p.value)
```


Fit the gamma distribution directly.

```{r}
gamma.mom <- function(x) {
  x.bar <- mean(x)
  n <- length(x)
  v <- var(x) * (n-1) / n
  
  l.est <- x.bar/v
  a.est<-(x.bar)^2/v
  # gamma distribution has mean shape*scale and variance shape*scale**2 + shape**2*scale**2
  
  return(list(rate=l.est, shape=a.est))
  
}
print(gamma.mom(Event.Intensities))
gamma.mom.estimates<- gamma.mom(Event.Intensities)
```

Ks- Test the method of moments gamma parameters against the known distribution
```{r}
KS.gamma.mom <- ks.test(Event.Intensities, "pgamma", shape=gamma.mom.estimates$shape, rate=gamma.mom.estimates$rate)
c(KS.gamma.mom$statistic, P.Value=KS.gamma$p.value)
```

This is an extremely low D statistic and p-value -- we can conclude that the intensities follow a gamma distribution. This implies that the counts follow a negative binomial distribution. We will try to estimate that negative binomial distribution in part II
